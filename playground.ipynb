{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fundamentals of Deep Learning -  Playground"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ§  Welcome to this module of Deep Learning!\n",
    "\n",
    "ğŸ¯ In this challenge, our goal is two-fold:\n",
    "1. Get a visual representation of Neural Networks\n",
    "2. Build a better intuition of what Neural Networks are doing\n",
    "\n",
    "ğŸ‘‰ We will use ***[Tensorflow Playground](https://playground.tensorflow.org/)***\n",
    "\n",
    "_(This first challenge does not require much coding_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification in Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) The data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "â“ Let's go to the [Playground](https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=spiral&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=2&seed=0.23545&showTestData=false&discretize=false&percTrainData=70&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false&regularization_hide=true&showTestData_hide=false&stepButton_hide=false&activation_hide=false&problem_hide=false&batchSize_hide=true&dataset_hide=false&resetButton_hide=false&discretize_hide=false&playButton_hide=false&learningRate_hide=true&regularizationRate_hide=true&percTrainData_hide=false&numHiddenLayers_hide=false) and select the following type of data â“ \n",
    "\n",
    "- A classification problem \n",
    "- The circle dataset (<span style=\"color:blue\">blue dots</span> inside a circle of <span style=\"color:orange\">orange dots</span>)\n",
    "- Ratio of training to test data : $ 70 \\% $\n",
    "- No noise ($ = 0$)\n",
    "- Do not show test data (right panel) \n",
    "- Do not discretize the output\n",
    "- Activation function: ***ReLU*** \n",
    "\n",
    "<details>\n",
    "    <summary><i> Why Relu? </i></summary>\n",
    "        \n",
    "ğŸ’¡ In general, try it by default. It appears to work better for many problems!\n",
    "    \n",
    "_Note: Playground only allows you to select **one** activation function that is used for **all** of the **hidden** layers_\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) The features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "â“ <u>Questions about the features</u> â“\n",
    "\n",
    "1. Select only the features $X_1$ and $X_2$ (_unselect the other features if necessary_)\n",
    "2. If you were using the other variables such as $X_1^{2}$, $X_2^{2}$, $X_1 X_2$, $sin(X_1)$ and $sin(X_2)$, what type of classic Machine Learning operation does it correspond to?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "source": [
    "> **YOUR ANSWER HERE**:  The operations described, like squaring features and using sine functions, are called \"feature engineering.\" It's a way to tweak or create new data to help machine learning models understand and predict better.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><i>Answer</i></summary>\n",
    "\n",
    "* It corresponds to some type of ***feature engineering*** where you transform them. \n",
    "    * <i>Examples: multiplication, sinus, square, ...</i>\n",
    "* Here, in this exercise but also tomorrow, we will only use the raw input features $X_1$ and $X_2$. \n",
    "</details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) Building and Fitting a Neural Network in ***Playground***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "â“ <u>Questions about Neural Networks</u> â“ \n",
    "\n",
    "* ğŸ§  Build a model with the following architecture:\n",
    "    - three hidden layers\n",
    "    - 5 neurons on the first hidden layer\n",
    "    - 4 neurons on the second hidden layer\n",
    "    - 3 neurons on the last hidden layer\n",
    "    - In ***Playground***, the output layer is not represented: \n",
    "        - For such binary classification task, keep in mind that it will automatically be a dense layer with 1 neuron activated by the sigmoid function $ \\large \\phi(z) = \\frac{1}{1 + e^{-z}} $\n",
    "\n",
    "* ğŸ’ª ***Fit it and stop the iterations when the loss function has stabilized.***\n",
    "\n",
    "* ğŸ‘€ Observe carefully:\n",
    "    - Look at the individual neurons and try to understand what each neuron has specialized for during the _.fit()_\n",
    "    - What do you think about the overall shape your results? Re-run the neural network with different activation functions to compare. Can you make it work with \"Linear\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Answer: some insights about the activation functions</summary>\n",
    "\n",
    "- Results may look like a hexagon because ReLu is piece-wise linear!\n",
    "- A non-linearly separable problem cannot be fitted with a linear activation such as **Linear**\n",
    "- Surprisingly, a piece-wise linear activation function such as **ReLu** (or **LeakyReLu**) fits this non-linearly separable problem well (even if that is not always true)\n",
    "- The `tanh` activation gives a \"smoother\" decision boundary\n",
    "- The **sigmoid** does **not** seem to work well here (i.e. it takes __many__ epochs before it can classify effectively)\n",
    "    \n",
    "ğŸ§‘ğŸ»â€ğŸ« Always start with ReLu, it's a safe bet ğŸ§‘ğŸ»â€ğŸ«!\n",
    "</details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) Building and Fitting a Neural Network in ***Tensorflow.Keras***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ‘‡ We wrote the same model for you - at least the architecture - in Tensorflow's Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "model = models.Sequential()\n",
    "\n",
    "model.add(layers.Dense(5, activation='relu', input_dim=2)) # 1st hidden layer with 5 neurons\n",
    "model.add(layers.Dense(4, activation='relu')) # 2nd hidden layer with 4 neurons\n",
    "model.add(layers.Dense(3, activation='relu')) # 3rd hidden layer with 3 neurons\n",
    "\n",
    "\n",
    "\n",
    "model.add(layers.Dense(1, activation='sigmoid')) # Output layer that outputs a probability of belonging\n",
    "                                                 # to the class of \"success\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><i>What to understand about the code of a Neural Network? </i>ğŸ‘†</summary>\n",
    "\n",
    "- <u>First Hidden Layer a.k.a ***Input Layer***</u>:\n",
    "    - Every datapoint that will be input to the neural network has two features $ X = \\begin{bmatrix} \n",
    "           X_{1} \\\\\n",
    "           X_{2} \\\\\n",
    "         \\end{bmatrix} $.\n",
    "    - You need to inform your Neural Network about the ***number of input features*** through the ***`input_dim` argument***\n",
    "    - A Neural Network tries to mimic the human brain. Here we would like to use 5 neurons to start analyzing each of these points.\n",
    "    \n",
    "    - Every datapoint goes through the first hidden layer which was built using 5 neurons $ layer_1 = \\begin{bmatrix} \n",
    "           a_{1} \\\\\n",
    "           a_{2} \\\\\n",
    "           a_{3} \\\\\n",
    "           a_{4} \\\\\n",
    "           a_{5} \\\\           \n",
    "         \\end{bmatrix} $\n",
    "    \n",
    " - <u>Second Hidden Layer</u>:\n",
    "         \n",
    "    - What if we want to ***make the information flow*** through a second hidden layer with 4 neurons? It is totally possible!\n",
    "    - These 4 neurons $ layer_2 = \\begin{bmatrix} \n",
    "           b_{1} \\\\\n",
    "           b_{2} \\\\\n",
    "           b_{3} \\\\\n",
    "           b_{4} \\\\ \n",
    "         \\end{bmatrix} $ from the second layer will analyze the output from the 5 neurons in the first layer\n",
    "    \n",
    "- <u>Third Hidden Layer</u>:\n",
    "        - What if we want the information to **continue to flow** through a third hidden layer with 3 neurons? Again, totally possible!\n",
    "\n",
    "    - Every neuron's output from the second layer goes through the third hidden layer which was built using 3 neurons $ layer_3 = \\begin{bmatrix} \n",
    "           c_{1} \\\\\n",
    "           c_{2} \\\\\n",
    "           c_{3} \n",
    "         \\end{bmatrix} $\n",
    "         \n",
    "    - These 3 neurons analyze the outputs of the neurons in $ layer_2  $ !\n",
    "\n",
    "- <u>Predictive Layer</u>\n",
    "    - You are dealing with a binary classification task\n",
    "    - We could use two neurons to predict the probability of belonging to class A or class B...\n",
    "    - But one neuron predicting the probability of \"success\" is enough\n",
    "\n",
    "- <u>About activation functions</u>\n",
    "    - Despite its simplicity, the ***ReLU*** has proven to be very effective to add some non-linearity to the layers\n",
    "    - For the predictive layer, the best activation function to use for a classification task is the ***sigmoid*** function. That is something we've already discussed during Decision Science and Machine Learning.\n",
    "\n",
    "- <u>About the Sequential aspect of the Network</u>:\n",
    "    - The fact that you are defining a **Sequential** model has a consequence: each layer is aware of its input size based on the output size of the previous layer!\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "â“ How many parameters are involved in this small Neural Network â“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)              â”‚            <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)              â”‚            <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              â”‚            <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span> â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ dense_8 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)              â”‚            \u001b[38;5;34m15\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_9 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)              â”‚            \u001b[38;5;34m24\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_10 (\u001b[38;5;33mDense\u001b[0m)                â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              â”‚            \u001b[38;5;34m15\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_11 (\u001b[38;5;33mDense\u001b[0m)                â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              â”‚             \u001b[38;5;34m4\u001b[0m â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">58</span> (232.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m58\u001b[0m (232.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">58</span> (232.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m58\u001b[0m (232.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "#Initilize\n",
    "model = Sequential()\n",
    "\n",
    "# Adding the first hidden layer with 5 neurons, input_dim=2 indicates two input features\n",
    "model.add(Dense(5, activation='relu', input_dim=2))\n",
    "\n",
    "model.add(Dense(4, activation='relu'))\n",
    "model.add(Dense(3, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><i>Hint</i></summary>\n",
    "\n",
    "âœ… You should have 58 parameters\n",
    "    \n",
    "âŒ If not, double-check your architecture    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5) The XOR Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "â“ <u>Playing with the XOR Dataset</u> â“ \n",
    "\n",
    "* On Playground:\n",
    "    - Change the dataset to the \"XOR - Exclusive Or\".\n",
    "    - Try to design a model with two hidden layers that has a very small **test loss** \n",
    "        - Note: you are free to choose the number of neurons per layer yourself.  \n",
    "        \n",
    "* Coding with Tensorflow/Keras:\n",
    "    - Once you have built your model on Playground, code it down below with the Tensorflow/Keras library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 1.0000 - loss: 0.1876\n",
      "Loss: 0.18757805228233337, Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Neural Network that can be well fitted to the XOR Dataset\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "X = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
    "y = np.array([[0], [1], [1], [0]])\n",
    "    \n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(8, input_dim=2, activation='relu'))\n",
    "model.add(Dense(4, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X, y, epochs=1000, verbose=0)\n",
    "loss, accuracy = model.evaluate(X, y)\n",
    "print(f\"Loss: {loss}, Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6) The Spiral Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "â“ <u>Playing with the Spiral Dataset</u> â“ \n",
    "\n",
    "* On Playground:\n",
    "    - Change the dataset to the \"Spiral\".\n",
    "    - Try to design a model with three hidden layers that has a very small **test loss** \n",
    "        - Note: you are free to choose the number of neurons per layer yourself.  \n",
    "        \n",
    "* Coding with Tensorflow/Keras:\n",
    "    - Once you have built your model on Playground, code it down  below with the Tensorflow/Keras library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6070 - loss: 0.6349 \n",
      "Loss: 0.6251989603042603, Accuracy: 0.6399999856948853\n"
     ]
    }
   ],
   "source": [
    "# Neural Network that can be well fitted to the Spiral Dataset\n",
    "\n",
    "pass \n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Spiral input and output data (example placeholders)\n",
    "X = np.random.rand(100, 2)  # Example feature data\n",
    "y = np.random.randint(0, 2, size=(100, 1))  # Example labels\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Add first hidden layer with N neurons (e.g., 16 neurons) and 'relu' activation function\n",
    "model.add(Dense(16, input_dim=2, activation='relu'))\n",
    "\n",
    "# Add second hidden layer with M neurons (e.g., 12 neurons)\n",
    "model.add(Dense(12, activation='relu'))\n",
    "\n",
    "# Add third hidden layer with P neurons (e.g., 8 neurons)\n",
    "model.add(Dense(8, activation='relu'))\n",
    "\n",
    "# Add output layer with 1 neuron and 'sigmoid' activation function for binary classification\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model with binary crossentropy loss and adam optimizer\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, y, epochs=1000, verbose=0)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X, y)\n",
    "print(f\"Loss: {loss}, Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (7) How Deep should a Neural Network be ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ‘€ If you compare the number of parameters needed to fit the Spiral Dataset vs. the XOR dataset, the former requires many more weights....\n",
    "\n",
    "ğŸ˜ƒ Actually, if your models are deep enough, you could potentially fit pretty much any pattern...\n",
    "\n",
    "---\n",
    "\n",
    "<details>\n",
    "    <summary><i>Should I create Very Deep Neural Networks? </i></summary>\n",
    "        \n",
    "<u>Examples:</u>\n",
    "    \n",
    "* Think about a human being. The more this person spends time coding in Python, the better he/she will get better at it!\n",
    "    \n",
    "* Think about a student. The more this person studies, the better he/she will pass exams. But sometimes students can study \"too much\" about a topic and forget about the global picture of a course....\n",
    "    \n",
    "<u>Lessons</u>\n",
    "    \n",
    "ğŸ§  For Deep Learning Models, the more layers they have, the more opportunities they will have to learn the patterns in the data...\n",
    "\n",
    "â—ï¸ The problem is about avoiding **overfitting** â—ï¸\n",
    "    \n",
    "â˜ ï¸ Add a good deal of noise and you _may_ see that your model will have learned \"too much\" about this noise. \n",
    "  \n",
    "    \n",
    "ğŸ“† The next lecture **Deep Learning > Optimizers, Loss, & Fitting** is dedicated to helping you understand which techniques we can use to prevent Deep Learning models from overfitting.\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "<details>\n",
    "    <summary><i>A picture of overfitting in Playground</i></summary>\n",
    "    \n",
    "<img src='https://wagon-public-datasets.s3.amazonaws.com/data-science-images/DL/playground-overfitting.png' width=700 style='margin:auto'>\n",
    "</details>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression in Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Let's try to complete a Regression Task using Deep Learning</u>\n",
    "\n",
    "\n",
    "This time, the last layer will no longer look like:  \n",
    "```python\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "```\n",
    "\n",
    "but instead  :\n",
    "```python\n",
    "model.add(layers.Dense(1, activation='linear'))\n",
    "```\n",
    "\n",
    "This means that the output of this network is no longer between $0$ and $1$ (probability) but between $ -\\infty$ and $+ \\infty$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "â“ <u>Playing with the Regression Dataset</u> â“ \n",
    "\n",
    "* On Playground:\n",
    "    - Change the dataset to the \"Regression\".\n",
    "    - Try to design a model that has a very small **test loss** \n",
    "        - Note: you are free to choose both the number of layers and the number of neurons per layer yourself \n",
    "        \n",
    "* Coding with Tensorflow/Keras:\n",
    "    - Once you have built your model on Playground, code it down  below with the Tensorflow/Keras library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 893us/step - loss: 0.0761\n",
      "Loss: 0.07486186176538467\n"
     ]
    }
   ],
   "source": [
    "# Neural Network that can be well fitted to the Regression Dataset\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Regression input and output data (example placeholders)\n",
    "X = np.random.rand(100, 2)  # Example feature data\n",
    "y = np.random.rand(100, 1)  # Example continuous output data\n",
    "\n",
    "# Create a Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add hidden layers with chosen number of neurons and 'relu' activation function\n",
    "model.add(Dense(16, input_dim=2, activation='relu'))  # Example first hidden layer\n",
    "model.add(Dense(12, activation='relu'))  # Example second hidden layer\n",
    "\n",
    "# Add output layer with 1 neuron and 'linear' activation function for regression\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "# Compile the model with mean squared error loss and adam optimizer\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, y, epochs=1000, verbose=0)\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X, y)\n",
    "print(f\"Loss: {loss}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ You are now ready to do the same things with Tensorflow's Keras directly!\n",
    "\n",
    "ğŸ’ª This was a Warm-Up about Neural Networks / Deep Learning Models... (even if, admittedly, our networks in this challenge were not so \"deep\"). \n",
    "\n",
    "\n",
    "ğŸ’¾ Don't forget to `git add/commit/push` your notebook...\n",
    "\n",
    "ğŸš€ ... and move on to the next challenge!"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
