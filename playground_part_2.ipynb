{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playground - Part II"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üéØ **Goal**: Get a better understanding of ***Neural Network hyperparameters***\n",
    "\n",
    "<hr>\n",
    "\n",
    "üëâ Open the [Playground](https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=3&seed=0.06711&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false&regularization_hide=false&regularizationRate_hide=false) again to learn more about Neural Networks. \n",
    "\n",
    "‚ùóÔ∏è Keep in mind that as the algorithm is stochastic, the results may differ from one run to another. For this reason, do not hesitate to re-run the algorithms multiple times to analyse the behavior of your Neural Networks and draw your conclusions accordingly.\n",
    "\n",
    "üïµüèª Let's explore the different items we have seen during the lecture:\n",
    "- **Batch Size**\n",
    "- **Regularization**\n",
    "- **Learning Rate**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) The batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Initial Question** ‚ùì Select the `circle dataset` (Classification). \n",
    "\n",
    "* Build a model with: \n",
    "    * one hidden layer with 3 neurons,\n",
    "    * a _learning rate_ equal to 0.03, \n",
    "    * and the _tanh_ activation function\n",
    "\n",
    "* Do not add any noise (=0).\n",
    "\n",
    "* Select a batch size of 30\n",
    "\n",
    "***Look at the convergence of the algorithm. Does it seem slow or fast?***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <i>Answer here</i> With a batch size of 30, the convergence seems relatively fast. The train loss and test loss decrease quickly over the first few epochs, indicating the model is learning the patterns in the data efficiently. \n",
    "\n",
    "Explanation:\n",
    "# A larger batch size (e.g., 30) allows the model to estimate the gradient of the loss function more accurately,\n",
    "# leading to faster and more stable convergence compared to smaller batch sizes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: What is happening with a batch size of 1** ‚ùì \n",
    "\n",
    "Now, run this neural network on the same dataset but... \n",
    "\n",
    "* with a batch-size of 1.\n",
    "* Make sure to run it for at least 150 epochs. \n",
    "\n",
    "***What do you notice about the train and test loss? What is the reason of this instability?***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <i>Answer here</i>\n",
    "\n",
    "With a batch size of 1, the train loss and test loss are very noisy and unstable. The losses fluctuate significantly from one epoch to another. This instability is caused by the high variance in the gradient estimates when using a small batch size. Each update is based on a single example, which can lead to erratic changes in the model's weights.\n",
    "\n",
    "\n",
    "\n",
    "# Explanation:\n",
    "# When the batch size is 1, the model performs stochastic gradient descent (SGD).\n",
    "# SGD updates the model's weights based on the gradient of a single example at a time.\n",
    "# This can cause the model to take noisy and inconsistent steps, resulting in unstable loss curves.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question/Observation** ‚ùì \n",
    "\n",
    "Now, you can see the effect of the _batch_size_ by reading the values of the train loss and test loss: pause the iterations and run it step by step (iteration per iteration) using the `\"Step\"` button (at the right side of the play/stop button)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <i>Answer here</i>\n",
    "\n",
    "\n",
    "By running the model step by step with a small batch size (e.g., 1), you can observe how the train loss and test loss change after each update. The losses will likely vary significantly from step to step, reflecting the high variance in the gradient estimates. This observation reinforces the idea that small batch sizes lead to noisy and unstable training.\n",
    "\n",
    "# Explanation:\n",
    "# Running the model step by step allows you to see the immediate effect of each weight update on the losses.\n",
    "# With a small batch size, the updates are based on individual examples, leading to high variance and unstable behavior.\n",
    "# In contrast, larger batch sizes provide more stable updates by averaging the gradients over multiple examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question about the lack of generalization** ‚ùì \n",
    "\n",
    "To once again observe the **lack of generalization**:\n",
    "* Select the `\"eXclusive OR\"(XOR)` dataset, \n",
    "* with a noise of 50,\n",
    "* Add a second hidden layer with again 8 neurons. \n",
    "\n",
    "***Try to fit your model once again... what do you expect?***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <i>Add your comments here</i> With a noise level of 50 and an additional hidden layer, I expect the model to overfit the training data. The increased complexity of the model (more layers and neurons) allows it to fit the noise in the training set, leading to poor generalization on the test set. The test loss will likely increase as the model starts to memorize the noise instead of learning the underlying patterns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùóÔ∏è With a smaller batch size, your model will end up overfitting faster... ‚ùóÔ∏è\n",
    "\n",
    "üëâ Although, let's keep ***`batch size = 1`*** for the next question and try to understand how to prevent overfitting using the strategy of `regularization`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question about regularization** ‚ùì \n",
    "\n",
    "Can we ***regularize*** our network to ***avoid overfitting***? \n",
    "\n",
    "* Keep the batch size to 1,\n",
    "* Add a `L2-regularization`,\n",
    "* Increase the power of this L2-regularization until it smooths out the decision boundary! \n",
    "Notice how the test loss doesn't increase anymore with the epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <i>Add your comments here</i> Yes, we can apply L2 regularization to the model to mitigate overfitting. By increasing the regularization strength, the model will be encouraged to learn simpler and more generalizable patterns. The L2 regularization adds a penalty term to the loss function, discouraging large weights and reducing the model's complexity. As a result, the decision boundary becomes smoother, and the test loss stops increasing with further training epochs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Questions with the spiral dataset** ‚ùì \n",
    "\n",
    "<u>Configuration</u>:\n",
    "\n",
    "* Select the `spiral dataset`,\n",
    "* Remove regularization,\n",
    "* Increase the `ratio of training to test data` to 80%. \n",
    "\n",
    "<u>Neural Network</u>: 3 hidden layers with:\n",
    "* 8 neurons on the first layer, \n",
    "* 7 neurons on the second layer,\n",
    "* 6 neurons on the third layer. \n",
    "\n",
    "<u>Experiment</u>:\n",
    "\n",
    "* Run the algorithm with a batch size of 30,\n",
    "* Make sure to run it for at least 1500 epochs,\n",
    "* Then, compare it to the same run but with a batch size of 1. \n",
    "\n",
    "You can check what happens on the train loss and test loss step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <i>Add your comments here</i> When using a batch size of 30 with the spiral dataset and the specified neural network architecture, the model will likely converge faster and achieve better generalization compared to a batch size of 1. The larger batch size allows for more stable gradient estimates and updates, leading to smoother loss curves and improved performance.\n",
    "However, with a batch size of 1, the training will be more noisy and unstable. The model may take longer to converge and might get stuck in suboptimal solutions. The test loss may fluctuate significantly during training, indicating poor generalization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) The learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go back to the <u>`circle dataset`</u>:\n",
    "* with no noise,\n",
    "* and a *ratio of training to test data* of 50%,\n",
    "* Use a batch size of 20. \n",
    "\n",
    "Use a <u>neural network</u> with:\n",
    "* one layer of 5 neurons,\n",
    "* no regularization, \n",
    "* and the tanh activation function\n",
    "\n",
    "‚ùì **Question about the learning rate** ‚ùì \n",
    "\n",
    "For each learning rate (from 0.0001 to 10), run the algorithm during 1000 epochs and report the values of the test loss in the list below. Then, plot the test loss with respect to the learning rates. \n",
    "\n",
    "‚ùóÔ∏è <u>Warning</u> ‚ùóÔ∏è When you change the learning rate, make sure to reinitialize the neural network (_circular arrow, left to the play/pause button_)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (3917147672.py, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn [1], line 7\u001b[0;36m\u001b[0m\n\u001b[0;31m    test_loss = [UNCOMMENT THIS CELL AND STORE YOUR LOSS VALUES IN THIS LIST]\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "learning_rates = [0.0001, 0.001, 0.01, 0.1, 1, 10]\n",
    "\n",
    "\n",
    "\n",
    "plt.plot(np.log(learning_rates), test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùóÔ∏è <u>Warning</u> ‚ùóÔ∏è Too low and too high learning rates both lead to a high test loss... but not for the same reasons!\n",
    "\n",
    "* A **low learning rate** helps a neural network converge in a similar fashion to a moderate learning rate but... way slower... i.e. more epochs would be needed!\n",
    "* A **high learning rate** makes the algorithm diverge completely.\n",
    "    - Try a learning rate $ \\alpha = 10 $ with 400 epochs, you should see the loss vary. This corresponds to the fact that the algorithms converge to *different local minima*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üèÅ Congratulations!\n",
    "\n",
    "üíæ Do not forget to `git add/commit/push` your notebook...\n",
    "\n",
    "üöÄ ... and move to the next challenge!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add . && git commit -m \"Completed \" && git push origin master\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
